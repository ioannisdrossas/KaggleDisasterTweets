{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "id": "initial_id",
    "ExecuteTime": {
     "end_time": "2024-07-19T18:34:28.666543Z",
     "start_time": "2024-07-19T18:34:28.633462Z"
    }
   },
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import csv\n",
    "import random\n",
    "from random import shuffle\n",
    "from nltk.corpus import wordnet\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# Load data\n",
    "training_data = pd.read_csv(\"train.csv\")\n",
    "testing_data = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Remove irrelevant columns\n",
    "columns_to_remove = [\"keyword\", \"location\"]\n",
    "training_data = training_data.drop(columns_to_remove, axis=1)\n",
    "testing_data = testing_data.drop(columns_to_remove, axis=1)"
   ],
   "metadata": {
    "id": "5ed506a061a13d9a",
    "ExecuteTime": {
     "end_time": "2024-07-19T18:34:28.685943Z",
     "start_time": "2024-07-19T18:34:28.639654Z"
    }
   },
   "id": "5ed506a061a13d9a"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# Convert text to lowercase\n",
    "training_data[\"text\"] = training_data[\"text\"].str.lower()\n",
    "testing_data[\"text\"] = testing_data[\"text\"].str.lower()"
   ],
   "metadata": {
    "id": "7117e2af059c726",
    "ExecuteTime": {
     "end_time": "2024-07-19T18:34:28.686159Z",
     "start_time": "2024-07-19T18:34:28.675339Z"
    }
   },
   "id": "7117e2af059c726"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def remove_hashtags(text):\n",
    "    return re.sub(r'#\\w+', '', text)\n",
    "\n",
    "def remove_tags(text):\n",
    "    return re.sub(r'@\\w+', '', text)\n",
    "\n",
    "def remove_urls(text):\n",
    "    return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "training_data['text'] = training_data['text'].apply(remove_hashtags).apply(remove_tags).apply(remove_urls)\n",
    "testing_data['text'] = testing_data['text'].apply(remove_hashtags).apply(remove_tags).apply(remove_urls)"
   ],
   "metadata": {
    "id": "aa34f0159c017947",
    "ExecuteTime": {
     "end_time": "2024-07-19T18:34:28.715339Z",
     "start_time": "2024-07-19T18:34:28.698417Z"
    }
   },
   "id": "aa34f0159c017947"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ioannisdrossas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "training_data['text'] = training_data['text'].apply(remove_stopwords)\n",
    "testing_data['text'] = testing_data['text'].apply(remove_stopwords)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c6bac6e8622c48c1",
    "outputId": "394bbd8c-1632-4dd7-e545-7cc5465b8564",
    "ExecuteTime": {
     "end_time": "2024-07-19T18:34:28.744711Z",
     "start_time": "2024-07-19T18:34:28.717775Z"
    }
   },
   "id": "c6bac6e8622c48c1"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "training_data['text'] = training_data['text'].apply(remove_punctuation)\n",
    "testing_data['text'] = testing_data['text'].apply(remove_punctuation)"
   ],
   "metadata": {
    "id": "463ed1ad7611058f",
    "ExecuteTime": {
     "end_time": "2024-07-19T18:34:28.781179Z",
     "start_time": "2024-07-19T18:34:28.750143Z"
    }
   },
   "id": "463ed1ad7611058f"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def correct_spelling(text):\n",
    "    return str(TextBlob(text).correct())\n",
    "\n",
    "for i in tqdm(range(len(training_data))):\n",
    "    text = training_data.at[i, 'text']\n",
    "    corrected_text = correct_spelling(text)\n",
    "    training_data.at[i, 'text'] = corrected_text\n",
    "\n",
    "for i in tqdm(range(len(testing_data))):\n",
    "    text = testing_data.at[i, 'text']\n",
    "    corrected_text = correct_spelling(text)\n",
    "    testing_data.at[i, 'text'] = corrected_text"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4d4dc65766e2b8a",
    "outputId": "cbb1752e-e25e-4fdf-cd6b-83a589a50671",
    "ExecuteTime": {
     "end_time": "2024-07-19T18:34:28.781723Z",
     "start_time": "2024-07-19T18:34:28.770359Z"
    }
   },
   "id": "b4d4dc65766e2b8a"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ioannisdrossas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ioannisdrossas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/ioannisdrossas/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "training_data['text'] = training_data['text'].apply(lemmatize_text)\n",
    "testing_data['text'] = testing_data['text'].apply(lemmatize_text)"
   ],
   "metadata": {
    "id": "cbd1339e5ea9f3fd",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1aefd818-a290-4a94-de92-a798e23416aa",
    "ExecuteTime": {
     "end_time": "2024-07-19T18:34:29.731406Z",
     "start_time": "2024-07-19T18:34:28.773650Z"
    }
   },
   "id": "cbd1339e5ea9f3fd"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "#taken from EDA [https://github.com/jasonwei20/eda_nlp/blob/master/code/eda.py]\n",
    "# Easy data augmentation techniques for text classification\n",
    "# Jason Wei and Kai Zou\n",
    "\n",
    "def synonym_replacement(words, n):\n",
    "\tnew_words = words.copy()\n",
    "\trandom_word_list = list(set([word for word in words if word not in stop_words]))\n",
    "\trandom.shuffle(random_word_list)\n",
    "\tnum_replaced = 0\n",
    "\tfor random_word in random_word_list:\n",
    "\t\tsynonyms = get_synonyms(random_word)\n",
    "\t\tif len(synonyms) >= 1:\n",
    "\t\t\tsynonym = random.choice(list(synonyms))\n",
    "\t\t\tnew_words = [synonym if word == random_word else word for word in new_words]\n",
    "\t\t\t#print(\"replaced\", random_word, \"with\", synonym)\n",
    "\t\t\tnum_replaced += 1\n",
    "\t\tif num_replaced >= n: #only replace up to n words\n",
    "\t\t\tbreak\n",
    "\n",
    "\t#this is stupid but we need it, trust me\n",
    "\tsentence = ' '.join(new_words)\n",
    "\tnew_words = sentence.split(' ')\n",
    "\n",
    "\treturn new_words\n",
    "\n",
    "def get_synonyms(word):\n",
    "\tsynonyms = set()\n",
    "\tfor syn in wordnet.synsets(word):\n",
    "\t\tfor l in syn.lemmas():\n",
    "\t\t\tsynonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "\t\t\tsynonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
    "\t\t\tsynonyms.add(synonym)\n",
    "\tif word in synonyms:\n",
    "\t\tsynonyms.remove(word)\n",
    "\treturn list(synonyms)\n",
    "\n",
    "########################################################################\n",
    "# Random deletion\n",
    "# Randomly delete words from the sentence with probability p\n",
    "########################################################################\n",
    "\n",
    "def random_deletion(words, p):\n",
    "\n",
    "\t#obviously, if there's only one word, don't delete it\n",
    "\tif len(words) == 1:\n",
    "\t\treturn words\n",
    "\n",
    "\t#randomly delete words with probability p\n",
    "\tnew_words = []\n",
    "\tfor word in words:\n",
    "\t\tr = random.uniform(0, 1)\n",
    "\t\tif r > p:\n",
    "\t\t\tnew_words.append(word)\n",
    "\n",
    "\t#if you end up deleting all words, just return a random word\n",
    "\tif len(new_words) == 0:\n",
    "\t\trand_int = random.randint(0, len(words)-1)\n",
    "\t\treturn [words[rand_int]]\n",
    "\n",
    "\treturn new_words\n",
    "\n",
    "########################################################################\n",
    "# Random swap\n",
    "# Randomly swap two words in the sentence n times\n",
    "########################################################################\n",
    "\n",
    "def random_swap(words, n):\n",
    "\tnew_words = words.copy()\n",
    "\tfor _ in range(n):\n",
    "\t\tnew_words = swap_word(new_words)\n",
    "\treturn new_words\n",
    "\n",
    "def swap_word(new_words):\n",
    "\trandom_idx_1 = random.randint(0, len(new_words)-1)\n",
    "\trandom_idx_2 = random_idx_1\n",
    "\tcounter = 0\n",
    "\twhile random_idx_2 == random_idx_1:\n",
    "\t\trandom_idx_2 = random.randint(0, len(new_words)-1)\n",
    "\t\tcounter += 1\n",
    "\t\tif counter > 3:\n",
    "\t\t\treturn new_words\n",
    "\tnew_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]\n",
    "\treturn new_words\n",
    "\n",
    "########################################################################\n",
    "# Random insertion\n",
    "# Randomly insert n words into the sentence\n",
    "########################################################################\n",
    "\n",
    "def random_insertion(words, n):\n",
    "\tnew_words = words.copy()\n",
    "\tfor _ in range(n):\n",
    "\t\tadd_word(new_words)\n",
    "\treturn new_words\n",
    "\n",
    "def add_word(new_words):\n",
    "\tsynonyms = []\n",
    "\tcounter = 0\n",
    "\twhile len(synonyms) < 1:\n",
    "\t\trandom_word = new_words[random.randint(0, max(0,len(new_words)-1))]\n",
    "\t\tsynonyms = get_synonyms(random_word)\n",
    "\t\tcounter += 1\n",
    "\t\tif counter >= 10:\n",
    "\t\t\treturn\n",
    "\trandom_synonym = synonyms[0]\n",
    "\trandom_idx = random.randint(0, len(new_words)-1)\n",
    "\tnew_words.insert(random_idx, random_synonym)\n",
    "\n",
    "########################################################################\n",
    "# main data augmentation function\n",
    "########################################################################\n",
    "\n",
    "def eda(sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=9):\n",
    "\twords = sentence.split(' ')\n",
    "\twords = [word for word in words if word != '']\n",
    "\tnum_words = len(words)\n",
    "\n",
    "\taugmented_sentences = []\n",
    "\tnum_new_per_technique = int(num_aug/4)+1\n",
    "\n",
    "\t#sr\n",
    "\tif (alpha_sr > 0):\n",
    "\t\tn_sr = max(1, int(alpha_sr*num_words))\n",
    "\t\tfor _ in range(num_new_per_technique):\n",
    "\t\t\ta_words = synonym_replacement(words, n_sr)\n",
    "\t\t\taugmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "\t#ri\n",
    "\tif (alpha_ri > 0):\n",
    "\t\tn_ri = max(1, int(alpha_ri*num_words))\n",
    "\t\tfor _ in range(num_new_per_technique):\n",
    "\t\t\ta_words = random_insertion(words, n_ri)\n",
    "\t\t\taugmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "\t#rs\n",
    "\tif (alpha_rs > 0):\n",
    "\t\tn_rs = max(1, int(alpha_rs*num_words))\n",
    "\t\tfor _ in range(num_new_per_technique):\n",
    "\t\t\ta_words = random_swap(words, n_rs)\n",
    "\t\t\taugmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "\t#rd\n",
    "\tif (p_rd > 0):\n",
    "\t\tfor _ in range(num_new_per_technique):\n",
    "\t\t\ta_words = random_deletion(words, p_rd)\n",
    "\t\t\taugmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "\tshuffle(augmented_sentences)\n",
    "\n",
    "\tif num_aug >= 1:\n",
    "\t\taugmented_sentences = augmented_sentences[:num_aug]\n",
    "\telse:\n",
    "\t\tkeep_prob = num_aug / len(augmented_sentences)\n",
    "\t\taugmented_sentences = [s for s in augmented_sentences if random.uniform(0, 1) < keep_prob]\n",
    "\n",
    "\taugmented_sentences.append(sentence)\n",
    "\n",
    "\treturn augmented_sentences"
   ],
   "metadata": {
    "id": "e47710da90bda44a",
    "ExecuteTime": {
     "end_time": "2024-07-19T18:34:29.742593Z",
     "start_time": "2024-07-19T18:34:29.740529Z"
    }
   },
   "id": "e47710da90bda44a"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for index, row in training_data.iterrows():\n",
    "    if len(row['text']) == 0:\n",
    "        continue\n",
    "    dummy = eda(row['text'], num_aug=3)\n",
    "\n",
    "    for i in dummy:\n",
    "        new_entry = pd.DataFrame([{'text': i, 'target': row['target']}])\n",
    "        training_data = pd.concat([training_data, new_entry], ignore_index=True)\n",
    "\n",
    "training_targets = training_data[\"target\"]\n",
    "training_data = training_data.drop(columns=[\"target\"], axis=1)"
   ],
   "metadata": {
    "id": "caf8178eafcc568c",
    "ExecuteTime": {
     "end_time": "2024-07-19T18:34:32.615791Z",
     "start_time": "2024-07-19T18:34:29.743449Z"
    }
   },
   "id": "caf8178eafcc568c"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ],
   "metadata": {
    "id": "95c4f430760a7c04",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8014c2c1-496f-4996-a3bc-c4b1511b28e8",
    "ExecuteTime": {
     "end_time": "2024-07-19T18:34:33.599921Z",
     "start_time": "2024-07-19T18:34:32.616405Z"
    }
   },
   "id": "95c4f430760a7c04"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "class DisasterDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.data.iloc[index]['text']\n",
    "        label = self.data.iloc[index]['target']\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "combined_data = pd.concat([training_data, training_targets], axis=1)\n",
    "train_data, val_data = train_test_split(combined_data, test_size=0.2, random_state=42)\n",
    "\n",
    "max_length = 128\n",
    "train_dataset = DisasterDataset(train_data, tokenizer, max_length)\n",
    "val_dataset = DisasterDataset(val_data, tokenizer, max_length)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ],
   "metadata": {
    "id": "84e801d7831512ba",
    "ExecuteTime": {
     "end_time": "2024-07-19T18:34:33.611200Z",
     "start_time": "2024-07-19T18:34:33.604014Z"
    }
   },
   "id": "84e801d7831512ba"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ioannisdrossas/Desktop/Kaggle/DisasterTweets/venv/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/10:   1%|          | 2/191 [00:31<49:52, 15.83s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[36], line 29\u001B[0m\n\u001B[1;32m     26\u001B[0m     loss \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mloss\n\u001B[1;32m     27\u001B[0m     total_train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m---> 29\u001B[0m     \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     30\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     32\u001B[0m avg_train_loss \u001B[38;5;241m=\u001B[39m total_train_loss \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(train_loader)\n",
      "File \u001B[0;32m~/Desktop/Kaggle/DisasterTweets/venv/lib/python3.10/site-packages/torch/_tensor.py:525\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    515\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    517\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    518\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    523\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    524\u001B[0m     )\n\u001B[0;32m--> 525\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    526\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    527\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Kaggle/DisasterTweets/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    262\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    266\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 267\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    275\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Kaggle/DisasterTweets/venv/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    742\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    743\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 744\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    745\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    746\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    747\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    748\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "# Initialize lists to store the training and validation loss\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Variable to store the best validation loss\n",
    "best_val_loss = float('inf')\n",
    "best_model_path = './best_model.pt'\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Average Training Loss: {avg_train_loss}')\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f'Validation Epoch {epoch + 1}/{num_epochs}'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Average Validation Loss: {avg_val_loss}')\n",
    "\n",
    "    # Save the model if the validation loss is the best we've seen so far.\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f'New best model saved at epoch {epoch + 1} with validation loss: {avg_val_loss}')\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('./fine_tuned_bert_model')\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "1f20607ccd09cf39",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 911
    },
    "outputId": "fa4a1c10-2aa4-4a75-dffe-a7301a11346d",
    "ExecuteTime": {
     "end_time": "2024-07-19T18:35:05.429420Z",
     "start_time": "2024-07-19T18:34:33.614488Z"
    }
   },
   "id": "1f20607ccd09cf39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "def predict(text, tokenizer, model, device, max_length=128):\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predicted_label = torch.argmax(logits, dim=1).cpu().item()\n",
    "\n",
    "    return predicted_label\n",
    "\n",
    "\n",
    "labels = []\n",
    "for index, row in tqdm(testing_data.iterrows(), total=testing_data.shape[0], desc=\"Processing rows\"):\n",
    "    text = row['text']\n",
    "    label = predict(text, tokenizer, model, device)\n",
    "    labels.append(label)\n",
    "\n",
    "# Store the results back in the DataFrame\n",
    "testing_data['label'] = labels"
   ],
   "metadata": {
    "id": "511a6ce264ae69da",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "84f0541c-ae21-40a6-ca32-e1e1adfa4026",
    "ExecuteTime": {
     "end_time": "2024-07-19T18:35:05.430956Z",
     "start_time": "2024-07-19T18:35:05.430777Z"
    }
   },
   "id": "511a6ce264ae69da"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filename = 'my_predictions.csv'\n",
    "\n",
    "predictions_list = testing_data['label'].tolist()\n",
    "index_list = testing_data['id'].tolist()\n",
    "\n",
    "with open(filename, 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "\n",
    "    # Write header row\n",
    "    csvwriter.writerow([\"id\", \"target\"])\n",
    "\n",
    "    # Write data rows\n",
    "    for index in range(5):\n",
    "        id = index_list[index]\n",
    "        prediction = predictions_list[index]\n",
    "        csvwriter.writerow([id, prediction])"
   ],
   "metadata": {
    "id": "5ee87f991f125051",
    "ExecuteTime": {
     "end_time": "2024-07-19T18:35:05.435280Z",
     "start_time": "2024-07-19T18:35:05.431560Z"
    }
   },
   "id": "5ee87f991f125051"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

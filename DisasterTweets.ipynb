{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "id": "initial_id"
   },
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import csv\n",
    "import random\n",
    "from random import shuffle\n",
    "from nltk.corpus import wordnet\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load data\n",
    "training_data = pd.read_csv(\"train.csv\")\n",
    "testing_data = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Remove irrelevant columns\n",
    "columns_to_remove = [\"keyword\", \"location\"]\n",
    "training_data = training_data.drop(columns_to_remove, axis=1)\n",
    "testing_data = testing_data.drop(columns_to_remove, axis=1)"
   ],
   "metadata": {
    "id": "5ed506a061a13d9a"
   },
   "id": "5ed506a061a13d9a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert text to lowercase\n",
    "training_data[\"text\"] = training_data[\"text\"].str.lower()\n",
    "testing_data[\"text\"] = testing_data[\"text\"].str.lower()"
   ],
   "metadata": {
    "id": "7117e2af059c726"
   },
   "id": "7117e2af059c726"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def remove_hashtags(text):\n",
    "    return re.sub(r'#\\w+', '', text)\n",
    "\n",
    "def remove_tags(text):\n",
    "    return re.sub(r'@\\w+', '', text)\n",
    "\n",
    "def remove_urls(text):\n",
    "    return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "training_data['text'] = training_data['text'].apply(remove_hashtags).apply(remove_tags).apply(remove_urls)\n",
    "testing_data['text'] = testing_data['text'].apply(remove_hashtags).apply(remove_tags).apply(remove_urls)"
   ],
   "metadata": {
    "id": "aa34f0159c017947"
   },
   "id": "aa34f0159c017947"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "training_data['text'] = training_data['text'].apply(remove_stopwords)\n",
    "testing_data['text'] = testing_data['text'].apply(remove_stopwords)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c6bac6e8622c48c1",
    "outputId": "394bbd8c-1632-4dd7-e545-7cc5465b8564"
   },
   "id": "c6bac6e8622c48c1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "training_data['text'] = training_data['text'].apply(remove_punctuation)\n",
    "testing_data['text'] = testing_data['text'].apply(remove_punctuation)"
   ],
   "metadata": {
    "id": "463ed1ad7611058f"
   },
   "id": "463ed1ad7611058f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def correct_spelling(text):\n",
    "#     return str(TextBlob(text).correct())\n",
    "# \n",
    "# for i in tqdm(range(len(training_data))):\n",
    "#     text = training_data.at[i, 'text']\n",
    "#     corrected_text = correct_spelling(text)\n",
    "#     training_data.at[i, 'text'] = corrected_text\n",
    "# \n",
    "# for i in tqdm(range(len(testing_data))):\n",
    "#     text = testing_data.at[i, 'text']\n",
    "#     corrected_text = correct_spelling(text)\n",
    "#     testing_data.at[i, 'text'] = corrected_text"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4d4dc65766e2b8a",
    "outputId": "cbb1752e-e25e-4fdf-cd6b-83a589a50671"
   },
   "id": "b4d4dc65766e2b8a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "training_data['text'] = training_data['text'].apply(lemmatize_text)\n",
    "testing_data['text'] = testing_data['text'].apply(lemmatize_text)"
   ],
   "metadata": {
    "id": "cbd1339e5ea9f3fd",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1aefd818-a290-4a94-de92-a798e23416aa"
   },
   "id": "cbd1339e5ea9f3fd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#taken from EDA [https://github.com/jasonwei20/eda_nlp/blob/master/code/eda.py]\n",
    "# Easy data augmentation techniques for text classification\n",
    "# Jason Wei and Kai Zou\n",
    "\n",
    "def synonym_replacement(words, n):\n",
    "\tnew_words = words.copy()\n",
    "\trandom_word_list = list(set([word for word in words if word not in stop_words]))\n",
    "\trandom.shuffle(random_word_list)\n",
    "\tnum_replaced = 0\n",
    "\tfor random_word in random_word_list:\n",
    "\t\tsynonyms = get_synonyms(random_word)\n",
    "\t\tif len(synonyms) >= 1:\n",
    "\t\t\tsynonym = random.choice(list(synonyms))\n",
    "\t\t\tnew_words = [synonym if word == random_word else word for word in new_words]\n",
    "\t\t\t#print(\"replaced\", random_word, \"with\", synonym)\n",
    "\t\t\tnum_replaced += 1\n",
    "\t\tif num_replaced >= n: #only replace up to n words\n",
    "\t\t\tbreak\n",
    "\n",
    "\t#this is stupid but we need it, trust me\n",
    "\tsentence = ' '.join(new_words)\n",
    "\tnew_words = sentence.split(' ')\n",
    "\n",
    "\treturn new_words\n",
    "\n",
    "def get_synonyms(word):\n",
    "\tsynonyms = set()\n",
    "\tfor syn in wordnet.synsets(word):\n",
    "\t\tfor l in syn.lemmas():\n",
    "\t\t\tsynonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "\t\t\tsynonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
    "\t\t\tsynonyms.add(synonym)\n",
    "\tif word in synonyms:\n",
    "\t\tsynonyms.remove(word)\n",
    "\treturn list(synonyms)\n",
    "\n",
    "########################################################################\n",
    "# Random deletion\n",
    "# Randomly delete words from the sentence with probability p\n",
    "########################################################################\n",
    "\n",
    "def random_deletion(words, p):\n",
    "\n",
    "\t#obviously, if there's only one word, don't delete it\n",
    "\tif len(words) == 1:\n",
    "\t\treturn words\n",
    "\n",
    "\t#randomly delete words with probability p\n",
    "\tnew_words = []\n",
    "\tfor word in words:\n",
    "\t\tr = random.uniform(0, 1)\n",
    "\t\tif r > p:\n",
    "\t\t\tnew_words.append(word)\n",
    "\n",
    "\t#if you end up deleting all words, just return a random word\n",
    "\tif len(new_words) == 0:\n",
    "\t\trand_int = random.randint(0, len(words)-1)\n",
    "\t\treturn [words[rand_int]]\n",
    "\n",
    "\treturn new_words\n",
    "\n",
    "########################################################################\n",
    "# Random swap\n",
    "# Randomly swap two words in the sentence n times\n",
    "########################################################################\n",
    "\n",
    "def random_swap(words, n):\n",
    "\tnew_words = words.copy()\n",
    "\tfor _ in range(n):\n",
    "\t\tnew_words = swap_word(new_words)\n",
    "\treturn new_words\n",
    "\n",
    "def swap_word(new_words):\n",
    "\trandom_idx_1 = random.randint(0, len(new_words)-1)\n",
    "\trandom_idx_2 = random_idx_1\n",
    "\tcounter = 0\n",
    "\twhile random_idx_2 == random_idx_1:\n",
    "\t\trandom_idx_2 = random.randint(0, len(new_words)-1)\n",
    "\t\tcounter += 1\n",
    "\t\tif counter > 3:\n",
    "\t\t\treturn new_words\n",
    "\tnew_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]\n",
    "\treturn new_words\n",
    "\n",
    "########################################################################\n",
    "# Random insertion\n",
    "# Randomly insert n words into the sentence\n",
    "########################################################################\n",
    "\n",
    "def random_insertion(words, n):\n",
    "\tnew_words = words.copy()\n",
    "\tfor _ in range(n):\n",
    "\t\tadd_word(new_words)\n",
    "\treturn new_words\n",
    "\n",
    "def add_word(new_words):\n",
    "\tsynonyms = []\n",
    "\tcounter = 0\n",
    "\twhile len(synonyms) < 1:\n",
    "\t\trandom_word = new_words[random.randint(0, max(0,len(new_words)-1))]\n",
    "\t\tsynonyms = get_synonyms(random_word)\n",
    "\t\tcounter += 1\n",
    "\t\tif counter >= 10:\n",
    "\t\t\treturn\n",
    "\trandom_synonym = synonyms[0]\n",
    "\trandom_idx = random.randint(0, len(new_words)-1)\n",
    "\tnew_words.insert(random_idx, random_synonym)\n",
    "\n",
    "########################################################################\n",
    "# main data augmentation function\n",
    "########################################################################\n",
    "\n",
    "def eda(sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=9):\n",
    "\twords = sentence.split(' ')\n",
    "\twords = [word for word in words if word != '']\n",
    "\tnum_words = len(words)\n",
    "\n",
    "\taugmented_sentences = []\n",
    "\tnum_new_per_technique = int(num_aug/4)+1\n",
    "\n",
    "\t#sr\n",
    "\tif (alpha_sr > 0):\n",
    "\t\tn_sr = max(1, int(alpha_sr*num_words))\n",
    "\t\tfor _ in range(num_new_per_technique):\n",
    "\t\t\ta_words = synonym_replacement(words, n_sr)\n",
    "\t\t\taugmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "\t#ri\n",
    "\tif (alpha_ri > 0):\n",
    "\t\tn_ri = max(1, int(alpha_ri*num_words))\n",
    "\t\tfor _ in range(num_new_per_technique):\n",
    "\t\t\ta_words = random_insertion(words, n_ri)\n",
    "\t\t\taugmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "\t#rs\n",
    "\tif (alpha_rs > 0):\n",
    "\t\tn_rs = max(1, int(alpha_rs*num_words))\n",
    "\t\tfor _ in range(num_new_per_technique):\n",
    "\t\t\ta_words = random_swap(words, n_rs)\n",
    "\t\t\taugmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "\t#rd\n",
    "\tif (p_rd > 0):\n",
    "\t\tfor _ in range(num_new_per_technique):\n",
    "\t\t\ta_words = random_deletion(words, p_rd)\n",
    "\t\t\taugmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "\tshuffle(augmented_sentences)\n",
    "\n",
    "\tif num_aug >= 1:\n",
    "\t\taugmented_sentences = augmented_sentences[:num_aug]\n",
    "\telse:\n",
    "\t\tkeep_prob = num_aug / len(augmented_sentences)\n",
    "\t\taugmented_sentences = [s for s in augmented_sentences if random.uniform(0, 1) < keep_prob]\n",
    "\n",
    "\taugmented_sentences.append(sentence)\n",
    "\n",
    "\treturn augmented_sentences"
   ],
   "metadata": {
    "id": "e47710da90bda44a"
   },
   "id": "e47710da90bda44a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for index, row in training_data.iterrows():\n",
    "    if len(row['text']) == 0:\n",
    "        continue\n",
    "    dummy = eda(row['text'], num_aug=0)\n",
    "\n",
    "    for i in dummy:\n",
    "        new_entry = pd.DataFrame([{'text': i, 'target': row['target']}])\n",
    "        training_data = pd.concat([training_data, new_entry], ignore_index=True)\n",
    "\n",
    "training_targets = training_data[\"target\"]\n",
    "training_data = training_data.drop(columns=[\"target\"], axis=1)"
   ],
   "metadata": {
    "id": "caf8178eafcc568c"
   },
   "id": "caf8178eafcc568c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ],
   "metadata": {
    "id": "95c4f430760a7c04",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8014c2c1-496f-4996-a3bc-c4b1511b28e8"
   },
   "id": "95c4f430760a7c04"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DisasterDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.data.iloc[index]['text']\n",
    "        label = self.data.iloc[index]['target']\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "combined_data = pd.concat([training_data, training_targets], axis=1)\n",
    "train_data, val_data = train_test_split(combined_data, test_size=0.2, random_state=42)\n",
    "\n",
    "max_length = 128\n",
    "train_dataset = DisasterDataset(train_data, tokenizer, max_length)\n",
    "val_dataset = DisasterDataset(val_data, tokenizer, max_length)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ],
   "metadata": {
    "id": "84e801d7831512ba"
   },
   "id": "84e801d7831512ba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "# Initialize lists to store the training and validation loss\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Variable to store the best validation loss\n",
    "best_val_loss = float('inf')\n",
    "best_model_path = './best_model.pt'\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Average Training Loss: {avg_train_loss}')\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f'Validation Epoch {epoch + 1}/{num_epochs}'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Average Validation Loss: {avg_val_loss}')\n",
    "\n",
    "    # Save the model if the validation loss is the best we've seen so far.\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f'New best model saved at epoch {epoch + 1} with validation loss: {avg_val_loss}')\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('./fine_tuned_bert_model')\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "1f20607ccd09cf39",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 911
    },
    "outputId": "fa4a1c10-2aa4-4a75-dffe-a7301a11346d"
   },
   "id": "1f20607ccd09cf39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "def predict(text, tokenizer, model, device, max_length=128):\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predicted_label = torch.argmax(logits, dim=1).cpu().item()\n",
    "\n",
    "    return predicted_label\n",
    "\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "labels = []\n",
    "for index, row in tqdm(testing_data.iterrows(), total=testing_data.shape[0], desc=\"Processing rows\"):\n",
    "    text = row['text']\n",
    "    label = predict(text, tokenizer, model, device)\n",
    "    labels.append(label)\n",
    "\n",
    "# Store the results back in the DataFrame\n",
    "testing_data['label'] = labels"
   ],
   "metadata": {
    "id": "511a6ce264ae69da",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "84f0541c-ae21-40a6-ca32-e1e1adfa4026"
   },
   "id": "511a6ce264ae69da"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filename = 'my_predictions.csv'\n",
    "\n",
    "predictions_list = testing_data['label'].tolist()\n",
    "index_list = testing_data['id'].tolist()\n",
    "\n",
    "with open(filename, 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "\n",
    "    # Write header row\n",
    "    csvwriter.writerow([\"id\", \"target\"])\n",
    "\n",
    "    # Write data rows\n",
    "    for index in range(3263):\n",
    "        id = index_list[index]\n",
    "        prediction = predictions_list[index]\n",
    "        csvwriter.writerow([id, prediction])"
   ],
   "metadata": {
    "id": "5ee87f991f125051"
   },
   "id": "5ee87f991f125051"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
